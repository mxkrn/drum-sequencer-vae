{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drum sequencer VAE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_loglevel(\"info\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "from itertools import islice\n",
    "import math\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from dsvae.data.loader import NoteSequenceDataLoader\n",
    "from dsvae.models.vae import VAE\n",
    "from dsvae.utils.hparams import HParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Interesting runs:\n",
    "- elated-serenity-195\n",
    "- magic-eon-196\n",
    "- proud-resonance-206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env\n",
    "# os.environ[\"DEBUG\"] = \"1\"\n",
    "\n",
    "# RUN_PATH = \"mxkrn/drum-sequencer-vae-dsvae/1hj953io\"\n",
    "# RUN_NAME = \"elated-serenity-195\"\n",
    "# RUN_NAME = \"magic-eon-196\"\n",
    "# RUN_PATH = \"mxkrn/drum-sequencer-vae-dsvae/a9zwogu7\"\n",
    "RUN_NAME = \"proud-resonance-206\"\n",
    "RUN_PATH = \"mxkrn/drum-sequencer-vae-dsvae/2vy0x403\"\n",
    "DEVICE = \"cuda:0\"\n",
    "ONNX = True\n",
    "\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = Path(os.environ[\"DATA_SOURCE_DIR\"])\n",
    "data_loader = NoteSequenceDataLoader(\n",
    "    path_to_data=path_to_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    split=\"test\",\n",
    "    file_shuffle=True,\n",
    "    pattern_shuffle=False,\n",
    "    scale_factor=1,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams(run_path):\n",
    "        # get hyperparameters\n",
    "    config = yaml.load(wandb.restore(f\"config.yaml\", run_path=run_path, replace=True), Loader=yaml.FullLoader)\n",
    "    hparams = {}\n",
    "    for k, v in config.items():\n",
    "        if not k in [\"_wandb\", \"wandb_version\"]:\n",
    "            if isinstance(v, dict):\n",
    "                hparams[k] = v[\"value\"]\n",
    "    hparams = HParams(hparams)\n",
    "    hparams.batch_size = BATCH_SIZE\n",
    "    return hparams\n",
    "\n",
    "def get_model(run_path: str, run_name: str) -> nn.Module:\n",
    "    # get hyperparameters\n",
    "    hparams = get_hparams(run_path)\n",
    "\n",
    "    # load model\n",
    "    path_to_state_dict = f\"outputs/models/{run_name}/latest.pt\"\n",
    "    load_path =  Path.cwd().parent / path_to_state_dict\n",
    "    save_path = Path(path_to_state_dict).with_suffix(\".onnx\")\n",
    "    _state_dict_io = wandb.restore(path_to_state_dict, run_path=RUN_PATH)\n",
    "    model = VAE(hparams)\n",
    "    model.load_state_dict(torch.load(_state_dict_io.name))\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    return model.eval()\n",
    "\n",
    "def get_onnx_model_path(run_name: str):\n",
    "    onnx_model_path = f\"outputs/models/{run_name}/{run_name}.onnx\"\n",
    "    load_path =  Path.cwd().parent / onnx_model_path\n",
    "    assert load_path.is_file()\n",
    "    model = onnx.load(load_path)\n",
    "    onnx.checker.check_model(model)\n",
    "    return load_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = get_hparams(RUN_PATH)\n",
    "ONNX_MODEL_PATH = get_onnx_model_path(RUN_NAME)\n",
    "model = get_model(RUN_PATH, RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We want to evaluate on several custom metrics:\n",
    "1. Missing notes\n",
    "2. Added notes\n",
    "\n",
    "We will evaluate these metrics in several different scenarios, we expect specific results for each scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher force ratio\n",
    "\n",
    "In this panel, we analyze what happens when we use different teacher force ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = torch.tensor(0.5)\n",
    "\n",
    "def get_onsets(tensor, threshold = THRESHOLD) -> torch.Tensor:\n",
    "    ones = torch.tensor(1., device=tensor.device)\n",
    "    zeros = torch.tensor(0., device=tensor.device)\n",
    "    onsets = torch.where(tensor[:, :, :9] >= threshold, ones, zeros)\n",
    "    return onsets\n",
    "\n",
    "def get_velocities(tensor) -> torch.Tensor:\n",
    "    return tensor[:, :, 9:18]\n",
    "\n",
    "def get_offsets(tensor) -> torch.Tensor:\n",
    "    return tensor[:, :, 18:27]\n",
    "\n",
    "def preprocess(pattern):\n",
    "    pattern = torch.transpose(pattern, 1, 2)\n",
    "    pattern = torch.flip(pattern, [0, 1])\n",
    "    return pattern.detach().cpu().numpy()\n",
    "\n",
    "def normalize(pattern):\n",
    "    max = np.max(pattern)\n",
    "    return np.divide(pattern, torch.full(pattern.shape, max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_predict(input: torch.Tensor, delta_z: torch.Tensor, note_dropout :torch.Tensor, onnx_model_path: str):\n",
    "    session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "    ort_inputs = {\n",
    "        'input': input.detach().cpu().numpy(),\n",
    "        'delta_z': delta_z.cpu().numpy(),\n",
    "        'note_dropout': note_dropout.unsqueeze(0).cpu().numpy()\n",
    "    }\n",
    "    output = session.run(None, ort_inputs)\n",
    "    return output[0], output[1], output[2], output[3], output[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target, fname, frame_index = next(iter(data_loader))\n",
    "delta_z = torch.zeros(hparams.latent_size, dtype=torch.float)\n",
    "note_dropout = torch.tensor(1., dtype=torch.float)\n",
    "\n",
    "onsets, velocities, offsets, z, z_loss = onnx_predict(input, delta_z, note_dropout, str(ONNX_MODEL_PATH))\n",
    "output = np.concatenate([onsets, velocities, offsets], axis=-1)\n",
    "output = torch.tensor(output, device=DEVICE, dtype=torch.float)\n",
    "output_onsets = get_onsets(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = OrderedDict()\n",
    "teacher_force_ratios = [1.0, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "data = [x for x in data_loader]\n",
    "num_data_points = len(data)\n",
    "sum_input_notes = defaultdict(list)\n",
    "sum_output_notes = defaultdict(list)\n",
    "sum_latent_vectors = defaultdict(list)\n",
    "sum_outputs = defaultdict(list)\n",
    "\n",
    "# num_data_points = 10\n",
    "\n",
    "for ratio in teacher_force_ratios:\n",
    "    output_data[ratio] = {}\n",
    "    output_data[ratio][\"input_onsets\"] = []\n",
    "    output_data[ratio][\"output_onsets\"] = []\n",
    "    print(f\"------------------------ RATIO = {ratio} ------------------------\")\n",
    "    \n",
    "    # input\n",
    "    delta_z = torch.zeros(\n",
    "        (hparams.latent_size), dtype=torch.float, device=DEVICE\n",
    "    )\n",
    "    teacher_force_ratio = torch.tensor(ratio, dtype=torch.float, device=DEVICE)\n",
    "    threshold = 0.5\n",
    "\n",
    "    missing_notes = []\n",
    "    added_notes = []\n",
    "    count = 0\n",
    "\n",
    "    for input, target, fname, frame_index in tqdm(islice(data, 2000)):\n",
    "        input = input.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        if ONNX:\n",
    "            onsets, velocities, offsets, z, z_loss = onnx_predict(input, delta_z, teacher_force_ratio, str(ONNX_MODEL_PATH))\n",
    "            output = np.concatenate([onsets, velocities, offsets], axis=-1)\n",
    "            output = torch.tensor(output, device=DEVICE, dtype=torch.float)\n",
    "        else:\n",
    "            onsets, velocities, offsets, z, z_loss = model(input, delta_z, teacher_force_ratio)\n",
    "            output = torch.cat((onsets, velocities, offsets), -1)\n",
    "\n",
    "        # get onsets\n",
    "        output_onsets = get_onsets(output)\n",
    "        input_onsets = get_onsets(input)\n",
    "        delta = torch.ne(output_onsets, input_onsets).int()\n",
    "        \n",
    "        # append to example osnets\n",
    "        if len(output_data[ratio][\"input_onsets\"]) < 4:\n",
    "            output_data[ratio][\"input_onsets\"].append(preprocess(input_onsets))\n",
    "        if len(output_data[ratio][\"output_onsets\"]) < 4:\n",
    "            output_data[ratio][\"output_onsets\"].append(preprocess(output_onsets))\n",
    "        \n",
    "        # get missing notes\n",
    "        missing = torch.mul(delta, input_onsets)\n",
    "        missing_notes.append(torch.sum(missing).item())\n",
    "        \n",
    "        # get added notes\n",
    "        added = torch.mul(delta, output_onsets)\n",
    "        added_notes.append(torch.sum(added).item())\n",
    "        \n",
    "        # add sum of notes\n",
    "        sum_input_notes[ratio].append(torch.sum(input_onsets).item())\n",
    "        sum_output_notes[ratio].append(torch.sum(output_onsets).item())\n",
    "        \n",
    "        # get z-coordinates\n",
    "        sum_latent_vectors[ratio].append(z)\n",
    "        sum_outputs[ratio].append(output)\n",
    "        \n",
    "        if count == 0:\n",
    "            total_added = added\n",
    "            total_missing = missing\n",
    "            count += 1\n",
    "        else:\n",
    "            total_added = torch.add(total_added, added)\n",
    "            total_missing = torch.add(total_missing, missing)\n",
    "    \n",
    "    output_data[ratio][\"total_added\"] = total_added.view(-1, total_added.shape[1]*total_added.shape[2])\n",
    "    output_data[ratio][\"total_missing\"] = total_missing.view(-1, total_missing.shape[1]*total_missing.shape[2])\n",
    "    heatmap_added = preprocess(total_added)\n",
    "    output_data[ratio][\"heatmap_added\"] = normalize(heatmap_added)\n",
    "    heatmap_missing = preprocess(total_missing)\n",
    "    output_data[ratio][\"heatmap_missing\"] = normalize(heatmap_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input - Output Comparison\n",
    "\n",
    "In the following plots, we compare side-by-side a random sub-sample of input and output onset patterns. We evaluate the outputs for different teacher force ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_pattern_pair(input, output):\n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(25, 3))\n",
    "#     axs[0].set_title(\"Input pattern\")\n",
    "#     sns.heatmap(input, ax=axs[0])\n",
    "#     axs[1].set_title(\"Output pattern\")\n",
    "#     sns.heatmap(output, ax=axs[1])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ratio in teacher_force_ratios:\n",
    "#     print(f\"------------------ TEACHER FORCE RATIO = {ratio} ------------------\\n\")\n",
    "#     input_onsets = output_data[ratio][\"input_onsets\"]\n",
    "#     output_onsets = output_data[ratio][\"output_onsets\"]\n",
    "    \n",
    "#     for i, onset in enumerate(input_onsets):\n",
    "#         plot_pattern_pair(onset, output_onsets[i])\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Added notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_data[teacher_force_ratios[count]][\"heatmap_added\"].mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "rows = 2\n",
    "cols = 3\n",
    "\n",
    "print(f\"------------------------ ADDED NOTES ------------------------\\n\")\n",
    "teacher_force_ratios = [1.0, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(30, 12))\n",
    "count = 0\n",
    "for i in range(len(teacher_force_ratios) // rows):\n",
    "    for j in range(len(teacher_force_ratios) // cols):\n",
    "        axs[j, i].set_title(f\"Teacher force ratio: {teacher_force_ratios[count]}\")\n",
    "        sns.histplot(output_data[teacher_force_ratios[count]][\"total_added\"].cpu().numpy()[0], ax=axs[j, i], bins=20, stat=\"count\")\n",
    "        count += 1\n",
    "        axs[j, i].set_xlabel(\"Number of notes\")\n",
    "        axs[j, i].set_ylim([0, 140])\n",
    "        # axs[j, i].set_yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(30, 10))\n",
    "count = 0\n",
    "\n",
    "for i in range(len(teacher_force_ratios) // rows):\n",
    "    for j in range(len(teacher_force_ratios) // cols):\n",
    "        axs[j, i].set_title(f\"Teacher force ratio: {teacher_force_ratios[count]}\")\n",
    "        sns.heatmap(output_data[teacher_force_ratios[count]][\"heatmap_added\"].mean(axis=0), ax=axs[j, i])\n",
    "        count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"------------------------ MISSING NOTES ------------------------\\n\")\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(30, 10))\n",
    "count = 0\n",
    "for i in range(len(teacher_force_ratios) // rows):\n",
    "    for j in range(len(teacher_force_ratios) // cols):\n",
    "        axs[j, i].set_title(f\"Teacher force ratio: {teacher_force_ratios[count]}\")\n",
    "        sns.histplot(output_data[teacher_force_ratios[count]][\"total_missing\"].cpu().numpy()[0], ax=axs[j, i], bins=40, stat=\"count\")\n",
    "        axs[j, i].set_ylim([0, 40])\n",
    "        count += 1\n",
    "        plt.xlabel(\"Number of notes\")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(30, 10))\n",
    "count = 0\n",
    "\n",
    "for i in range(len(teacher_force_ratios) // rows):\n",
    "    for j in range(len(teacher_force_ratios) // cols):\n",
    "        axs[j, i].set_title(f\"Teacher force ratio: {teacher_force_ratios[count]}\")\n",
    "        sns.heatmap(output_data[teacher_force_ratios[count]][\"heatmap_missing\"].mean(axis=0), ax=axs[j, i])\n",
    "        count += 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average number of notes anomalies per ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_missing = []\n",
    "average_added = []\n",
    "for i in range(len(teacher_force_ratios)):\n",
    "    tensor = output_data[teacher_force_ratios[i]][\"heatmap_missing\"]\n",
    "    average_missing.append(torch.sum(tensor).item())\n",
    "    \n",
    "    tensor = output_data[teacher_force_ratios[i]][\"heatmap_added\"]\n",
    "    average_added.append(torch.sum(tensor).item())\n",
    "    \n",
    "fig, axs = plt.subplots(2, figsize=(15, 8))\n",
    "axs[0].set_title(\"Average number of missing notes per ratio\")\n",
    "axs[0].plot(teacher_force_ratios, average_missing)\n",
    "axs[1].set_xlabel(\"Teacher force ratio\")\n",
    "axs[0].set_ylabel(\"Number of notes\")\n",
    "\n",
    "axs[1].plot(teacher_force_ratios, average_added)\n",
    "axs[1].set_title(\"Average number of added notes per ratio\")\n",
    "axs[1].set_ylabel(\"Number of notes\")\n",
    "axs[1].set_xlabel(\"Teacher force ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimension1 = defaultdict(list)\n",
    "latent_dimension2 = defaultdict(list)\n",
    "# latent_dimension3 = defaultdict(list)\n",
    "# latent_dimension4 = defaultdict(list)\n",
    "for ratio in teacher_force_ratios:\n",
    "    for batch in sum_latent_vectors[ratio]:\n",
    "        latent_dimension1[ratio].append(batch[:, 0][0].item())\n",
    "        latent_dimension2[ratio].append(batch[:, 1][0].item())\n",
    "#         latent_dimension3[ratio].append(batch[:, 2][0].item())\n",
    "#         latent_dimension4[ratio].append(batch[:, 3][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for ratio in teacher_force_ratios:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    print(f\"---------------------- RATIO: {ratio} ----------------------\")\n",
    "    axs[0].set_title(\"Distribution of notes in input patterns\")\n",
    "    axs[0].hist(sum_input_notes[ratio], bins=20)\n",
    "    axs[0].set_xlabel(\"Number of notes\")\n",
    "    axs[1].set_title(\"Distribution of notes in output patterns\")\n",
    "    axs[1].hist(sum_output_notes[ratio], bins=20)\n",
    "    axs[1].set_xlabel(\"Number of notes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in teacher_force_ratios:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    print(f\"---------------------- RATIO: {ratio} ----------------------\")\n",
    "    axs[0].set_title(\"Distribution of latent dimension 1\")\n",
    "    axs[0].hist(latent_dimension1[ratio], bins=20)\n",
    "    axs[1].set_title(\"Distribution of latent dimension 2\")\n",
    "    axs[1].hist(latent_dimension2[ratio], bins=20)\n",
    "#     axs[2].set_title(\"Distribution of latent dimension 3\")\n",
    "#     axs[2].hist(latent_dimension3[ratio], bins=20)\n",
    "#     axs[3].set_title(\"Distribution of latent dimension 4\")\n",
    "#     axs[3].hist(latent_dimension4[ratio], bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent space analysis\n",
    "\n",
    "1. Teacher force ratio = 1.0\n",
    "2. Teacher force ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "data_loader = NoteSequenceDataLoader(\n",
    "    path_to_data=path_to_data,\n",
    "    batch_size=1,\n",
    "    split=\"test\",\n",
    "    file_shuffle=True,\n",
    "    pattern_shuffle=False,\n",
    "    scale_factor=1,\n",
    "    num_workers=0\n",
    ")\n",
    "# assert sample.shape == (1, 16, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sigmoid normalization\n",
    "def sigmoid(array, gradient=1):\n",
    "    sigmoid = 1 / (1 + np.exp(np.multiply(array, -gradient)))\n",
    "    curve = np.subtract(sigmoid, 0.5)\n",
    "    return np.multiply(curve, 2)\n",
    "\n",
    "def construct_delta_z(i: int, j: int, dims = [0, 1], num_dims: int = 4, std: float = 1):\n",
    "    tensor = np.zeros(num_dims)\n",
    "    for d, _ in enumerate(range(num_dims)):\n",
    "        if d == dims[0]:\n",
    "            tensor[d] = (i / num_dims)*std\n",
    "        elif i == dims[1]:\n",
    "            tensor[d] = (j / num_dims)*std\n",
    "        else:\n",
    "            tensor[d] = 0.\n",
    "    \n",
    "    return torch.tensor(tensor, dtype=torch.float)\n",
    "\n",
    "def plot_pattern(model_output):\n",
    "    pass\n",
    "\n",
    "# test sigmoid creation\n",
    "x = np.linspace(0, 1, 1000)\n",
    "z = sigmoid(x, 5)\n",
    "  \n",
    "plt.plot(x, z) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"Sigmoid(X)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio = torch.tensor(0.4)\n",
    "\n",
    "# # get input\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(40, 5))\n",
    "# print('\\nINPUT\\n')\n",
    "# sample, target, filename, frame_index = next(iter(data_loader))\n",
    "# repeated_sample = sample.repeat(4, 0, 0)\n",
    "# sample = sample.to(DEVICE)\n",
    "# input_onsets = get_onsets(sample)\n",
    "# input_onsets = preprocess(input_onsets)\n",
    "# input_velocities = get_velocities(target)\n",
    "# input_velocities = preprocess(input_velocities)\n",
    "# input_offsets = get_offsets(target)\n",
    "# input_offsets = preprocess(input_offsets)\n",
    "\n",
    "# # input_velocities *= input_onsets\n",
    "# # print(input_velocities)\n",
    "# sns.heatmap(input_onsets, ax=axs[0])\n",
    "# sns.heatmap(input_velocities, ax=axs[1], vmin=0, vmax=1)\n",
    "# input_offsets *= input_onsets\n",
    "\n",
    "# sns.heatmap(input_offsets,vmin=-1,vmax=1, ax=axs[2], cmap=\"coolwarm\")\n",
    "# plt.show()\n",
    "\n",
    "# patterns = []\n",
    "# x_size, y_size = (6, 3)\n",
    "# std = 0.5\n",
    "\n",
    "\n",
    "# for i in range(x_size):\n",
    "#     fig, axs = plt.subplots(1, y_size, figsize=(40, 5))\n",
    "#     print(f'\\nOUTPUT {i}\\n')\n",
    "#     subpatterns = []\n",
    "#     delta_z = construct_delta_z(i, 1, num_dims=model.latent_size, std=0.5)\n",
    "#     axs[0].set_title(f\"Onsets - {delta_z}\")\n",
    "#     axs[1].set_title(f\"Velocities - {delta_z}\")\n",
    "#     axs[2].set_title(f\"Offsets - {delta_z}\")\n",
    "\n",
    "#     # print(delta_z)\n",
    "#     output, z, z_loss = model(sample, delta_z, ratio)\n",
    "    \n",
    "#     # onsets\n",
    "#     output_onsets = get_onsets(output)\n",
    "#     output_onsets = preprocess(output_onsets)\n",
    "    \n",
    "#     # velocities\n",
    "#     output_velocities = get_velocities(output)\n",
    "#     output_velocities = preprocess(output_velocities)\n",
    "#     output_velocities = sigmoid(output_velocities, 5)\n",
    "#     output_velocities *= output_onsets\n",
    "    \n",
    "#     # offsets\n",
    "#     output_offsets = get_offsets(output)\n",
    "#     output_offsets = preprocess(output_offsets)\n",
    "#     output_offsets = sigmoid(output_offsets, 8)\n",
    "#     output_offsets *= output_onsets\n",
    "    \n",
    "#     # plot\n",
    "#     sns.heatmap(output_onsets, ax=axs[0])\n",
    "#     sns.heatmap(output_velocities, ax=axs[1], vmin=0, vmax=1)\n",
    "#     sns.heatmap(output_offsets, vmin=-1, vmax=1, ax=axs[2], cmap=\"coolwarm\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate application side inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = torch.tensor(0.4)\n",
    "\n",
    "sample, target, filename, frame_index = next(iter(data_loader))\n",
    "sample = sample.to(DEVICE)\n",
    "\n",
    "# get inputs, velocities, offsets\n",
    "input_onsets = get_onsets(sample)\n",
    "input_onsets = preprocess(input_onsets)\n",
    "input_velocities = get_velocities(target)\n",
    "input_velocities = preprocess(input_velocities)\n",
    "input_offsets = get_offsets(target)\n",
    "input_offsets = preprocess(input_offsets)\n",
    "input_offsets *= input_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(40, 5))\n",
    "sns.heatmap(input_onsets[0], ax=axs[0])\n",
    "sns.heatmap(input_velocities[0], ax=axs[1], vmin=0, vmax=1)\n",
    "sns.heatmap(input_offsets[0], vmin=-1, vmax=1, ax=axs[2], cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    input: torch.Tensor,\n",
    "    delta_z: torch.Tensor,\n",
    "    onset_threshold_range: Tuple[float, float],\n",
    "    note_dropout_range: Tuple[float, float],\n",
    "    onnx_model_path: str,\n",
    "    output_size: int = 10,\n",
    "    velocity_scaling: int = 5, \n",
    "    offset_scaling: int = 5\n",
    "):\n",
    "    session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "    \n",
    "    input_grid = input.repeat(output_size, output_size, 1, 1)\n",
    "    assert input_grid.shape[-1] == input.shape[-1]\n",
    "    assert input_grid.shape[-2] == input.shape[-2]\n",
    "    \n",
    "    onset_thresholds = torch.linspace(onset_threshold_range[0], onset_threshold_range[1], output_size)\n",
    "    note_dropouts = torch.linspace(note_dropout_range[0], note_dropout_range[1], output_size)\n",
    "    \n",
    "    onsets = np.zeros((output_size, output_size, 9,  input.shape[-2]))\n",
    "    velocities = np.zeros((output_size, output_size, 9, input.shape[-2]))\n",
    "    offsets = np.zeros((output_size, output_size, 9, input.shape[-2]))\n",
    "    onset_threshold_lookup = np.zeros((output_size, output_size))\n",
    "    note_dropout_lookup = np.zeros((output_size, output_size))\n",
    "    \n",
    "    for i, input_column in enumerate(input_grid):\n",
    "        # predict\n",
    "        onset_threshold = onset_thresholds[i]\n",
    "        onset_threshold_lookup[i, :] = onset_threshold\n",
    "        note_dropout = note_dropouts[i]\n",
    "        note_dropout_lookup[i, :] = note_dropout\n",
    "        ort_inputs = {\n",
    "            \"input\": input_column.detach().cpu().numpy(),\n",
    "            \"delta_z\": delta_z.cpu().numpy(),\n",
    "            \"note_dropout\": note_dropout.unsqueeze(0).cpu().numpy()\n",
    "        }\n",
    "        _onsets, _velocities, _offsets, z, z_loss = session.run(None, ort_inputs)\n",
    "        output = np.concatenate([_onsets, _velocities, _offsets], axis=-1)\n",
    "        output = torch.tensor(output, dtype=torch.float)\n",
    "        \n",
    "        # onsets\n",
    "        output_onsets = get_onsets(output, onset_threshold)\n",
    "        output_onsets = preprocess(output_onsets)\n",
    "        onsets[i, :] = output_onsets\n",
    "\n",
    "        # velocities\n",
    "        output_velocities = get_velocities(output)\n",
    "        output_velocities = preprocess(output_velocities)\n",
    "        output_velocities = sigmoid(output_velocities, velocity_scaling)\n",
    "        velocities[i, :] = output_velocities\n",
    "\n",
    "        # offsets\n",
    "        output_offsets = get_offsets(output)\n",
    "        output_offsets = preprocess(output_offsets)\n",
    "        output_offsets = sigmoid(output_offsets, offset_scaling)\n",
    "        offsets[i, :] = output_offsets\n",
    "\n",
    "    return onsets, velocities, offsets, onset_threshold_lookup, note_dropout_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs, velocities, offsets\n",
    "input_onsets = get_onsets(sample)\n",
    "input_onsets = preprocess(input_onsets)\n",
    "input_velocities = get_velocities(target)\n",
    "input_velocities = preprocess(input_velocities)\n",
    "input_offsets = get_offsets(target)\n",
    "input_offsets = preprocess(input_offsets)\n",
    "input_offsets *= input_onsets\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(40, 5))\n",
    "sns.heatmap(input_onsets[0], ax=axs[0])\n",
    "sns.heatmap(input_velocities[0], ax=axs[1], vmin=0, vmax=1)\n",
    "sns.heatmap(input_offsets[0], vmin=-1, vmax=1, ax=axs[2], cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONSET_THRESHOLD_RANGE = [0.3, 0.7]\n",
    "NOTE_DROPOUT_RANGE = [0.7, 0.7]\n",
    "onsets, velocities, offsets, onset_thresholds, note_dropouts = predict(sample, torch.zeros(model.latent_size), ONSET_THRESHOLD_RANGE, NOTE_DROPOUT_RANGE, str(ONNX_MODEL_PATH))\n",
    "\n",
    "# plot\n",
    "axes_size = 10\n",
    "fig, axs = plt.subplots(axes_size, axes_size, figsize=(40, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(axes_size):\n",
    "    for j in range(axes_size):\n",
    "        axs[i, j].set_title(f\"Threshold: {round(onset_thresholds[i, j], 3)} | Dropout: {round(note_dropouts[i, j], 3)}\")\n",
    "        sns.heatmap(onsets[i][j], ax=axs[i, j])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].set_title(\"Onset thresholds\")\n",
    "sns.heatmap(onset_thresholds, ax=axs[0])\n",
    "axs[1].set_title(\"Note dropouts\")\n",
    "sns.heatmap(note_dropouts, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONSET_THRESHOLD_RANGE = [0.3, 0.7]\n",
    "NOTE_DROPOUT_RANGE = [0.5, 0.5]\n",
    "onsets, velocities, offsets, onset_thresholds, note_dropouts = predict(sample, torch.zeros(model.latent_size), ONSET_THRESHOLD_RANGE, NOTE_DROPOUT_RANGE, str(ONNX_MODEL_PATH))\n",
    "\n",
    "# plot\n",
    "axes_size = 10\n",
    "fig, axs = plt.subplots(axes_size, axes_size, figsize=(40, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(axes_size):\n",
    "    for j in range(axes_size):\n",
    "        axs[i, j].set_title(f\"Threshold: {round(onset_thresholds[i, j], 3)} | Dropout: {round(note_dropouts[i, j], 3)}\")\n",
    "        sns.heatmap(onsets[i][j], ax=axs[i, j])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].set_title(\"Onset thresholds\")\n",
    "sns.heatmap(onset_thresholds, ax=axs[0])\n",
    "axs[1].set_title(\"Note dropouts\")\n",
    "sns.heatmap(note_dropouts, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONSET_THRESHOLD_RANGE = [0.3, 0.7]\n",
    "NOTE_DROPOUT_RANGE = [0.7, 0.7]\n",
    "onsets, velocities, offsets, onset_thresholds, note_dropouts = predict(sample, torch.zeros(model.latent_size), ONSET_THRESHOLD_RANGE, NOTE_DROPOUT_RANGE, str(ONNX_MODEL_PATH))\n",
    "\n",
    "# plot\n",
    "axes_size = 10\n",
    "fig, axs = plt.subplots(axes_size, axes_size, figsize=(40, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(axes_size):\n",
    "    for j in range(axes_size):\n",
    "        axs[i, j].set_title(f\"Threshold: {round(onset_thresholds[i, j], 3)}\")\n",
    "        sns.heatmap(onsets[i][j], ax=axs[i, j])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].set_title(\"Onset thresholds\")\n",
    "sns.heatmap(onset_thresholds, ax=axs[0])\n",
    "axs[1].set_title(\"Note dropouts\")\n",
    "sns.heatmap(note_dropouts, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONSET_THRESHOLD_RANGE = [0.3, 0.7]\n",
    "NOTE_DROPOUT_RANGE = [0.9, 0.9]\n",
    "onsets, velocities, offsets, onset_thresholds, note_dropouts = predict(sample, torch.zeros(model.latent_size), ONSET_THRESHOLD_RANGE, NOTE_DROPOUT_RANGE, str(ONNX_MODEL_PATH))\n",
    "\n",
    "# plot\n",
    "axes_size = 10\n",
    "fig, axs = plt.subplots(axes_size, axes_size, figsize=(40, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(axes_size):\n",
    "    for j in range(axes_size):\n",
    "        axs[i, j].set_title(f\"Threshold: {round(onset_thresholds[i, j], 3)}\")\n",
    "        sns.heatmap(onsets[i][j], ax=axs[i, j])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].set_title(\"Onset thresholds\")\n",
    "sns.heatmap(onset_thresholds, ax=axs[0])\n",
    "axs[1].set_title(\"Note dropouts\")\n",
    "sns.heatmap(note_dropouts, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONSET_THRESHOLD_RANGE = [0.1, 0.7]\n",
    "NOTE_DROPOUT_RANGE = [0.9, 0.9]\n",
    "onsets, velocities, offsets, onset_thresholds, note_dropouts = predict(sample, torch.zeros(model.latent_size), ONSET_THRESHOLD_RANGE, NOTE_DROPOUT_RANGE, str(ONNX_MODEL_PATH))\n",
    "\n",
    "# plot\n",
    "axes_size = 10\n",
    "fig, axs = plt.subplots(axes_size, axes_size, figsize=(40, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(axes_size):\n",
    "    for j in range(axes_size):\n",
    "        axs[i, j].set_title(f\"Threshold: {round(onset_thresholds[i, j], 3)}\")\n",
    "        sns.heatmap(onsets[i][j], ax=axs[i, j])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].set_title(\"Onset thresholds\")\n",
    "sns.heatmap(onset_thresholds, ax=axs[0])\n",
    "axs[1].set_title(\"Note dropouts\")\n",
    "sns.heatmap(note_dropouts, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "axes_size = 10\n",
    "fig, axs = plt.subplots(axes_size, axes_size, figsize=(40, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(axes_size):\n",
    "    for j in range(axes_size):\n",
    "        axs[i, j].set_title(f\"Threshold: {round(onset_thresholds[i, j], 3)}\")\n",
    "        sns.heatmap(velocities[i][j], ax=axs[i, j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "axes_size = 10\n",
    "fig, axs = plt.subplots(axes_size, axes_size, figsize=(40, 20))\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(axes_size):\n",
    "    for j in range(axes_size):\n",
    "        axs[i, j].set_title(f\"Threshold: {round(onset_thresholds[i, j], 3)}\")\n",
    "        sns.heatmap(offsets[i][j], ax=axs[i, j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
